running: {'--uuid': 'f83e600d2a0051d2a908334f1b0f09a7', '-db-root': '/home/ryedida/smoothness-hpo/bbo_challenge_starter_kit/output', '--opt-root': '/home/ryedida/smoothness-hpo/bbo_challenge_starter_kit/example_submissions', '--data-root': None, '--db': 'run_20240113_043333', '--opt': 'random', '--data': 'breast', '--classifier': 'MLP-adam', '--metric': 'acc', '--calls': 15, '--suggestions': 1, '--jobs-file': None, '--verbose': False, 'dry_run': False, 'rev': 'a376313', 'opt_rev': '0.0.8'}
cmd: python /home/ryedida/.local/bin/bayesmark-exp -c MLP-adam -d breast -o random -u f83e600d2a0051d2a908334f1b0f09a7 -m acc -n 15 -p 1 -dir /home/ryedida/smoothness-hpo/bbo_challenge_starter_kit/output -b run_20240113_043333
/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

computed signature: ([{'alpha': 0.019628224813442792, 'batch_size': 182, 'beta_1': 0.9410202200271762, 'beta_2': 0.9998021557676793, 'epsilon': 1.8662266976518e-08, 'hidden_layer_sizes': 147, 'learning_rate_init': 0.0005627932047415167, 'tol': 0.03690557729213761, 'validation_fraction': 0.8846827852548593}, {'alpha': 0.001998246739232944, 'batch_size': 200, 'beta_1': 0.919111482530466, 'beta_2': 0.9998488260436156, 'epsilon': 5.981221901152555e-07, 'hidden_layer_sizes': 61, 'learning_rate_init': 2.2310905607443014e-05, 'tol': 1.2046852412030316e-05, 'validation_fraction': 0.8117896445826539}, {'alpha': 0.46659545670218433, 'batch_size': 219, 'beta_1': 0.9889789783750891, 'beta_2': 0.9999896868093284, 'epsilon': 2.4234724484675948e-08, 'hidden_layer_sizes': 167, 'learning_rate_init': 2.972334644335654e-05, 'tol': 0.0036281404040243792, 'validation_fraction': 0.17260651658522078}, {'alpha': 4.656005689076002, 'batch_size': 135, 'beta_1': 0.870503881627747, 'beta_2': 0.9948873266941017, 'epsilon': 2.1023308743480125e-07, 'hidden_layer_sizes': 118, 'learning_rate_init': 0.001878173875716191, 'tol': 1.1889379831773006e-05, 'validation_fraction': 0.6264327093752792}, {'alpha': 0.04705159350400542, 'batch_size': 158, 'beta_1': 0.9870884262730957, 'beta_2': 0.9999596874382349, 'epsilon': 1.1981845126013875e-08, 'hidden_layer_sizes': 116, 'learning_rate_init': 0.006173405204074311, 'tol': 1.7414134181586194e-05, 'validation_fraction': 0.6754299026638921}], [-0.676923076923077, -0.567032967032967, -0.545054945054945, -0.7824175824175824, -0.843956043956044])
/home/ryedida/.local/lib/python3.8/site-packages/bayesmark/experiment.py:469: UserWarning: Baselines not found. Will not log intermediate scores.
  warnings.warn("Baselines not found. Will not log intermediate scores.")

starting sklearn study random MLP-adam breast acc 15 1
with data root: None
suggestion time taken 0.004230 iter 0 next_points [{'alpha': 1.6141399233527927e-05, 'batch_size': 120, 'beta_1': 0.9118313465899606, 'beta_2': 0.9991836180301912, 'epsilon': 3.806099135654774e-09, 'hidden_layer_sizes': 187, 'learning_rate_init': 0.0026192605764600997, 'tol': 0.00014664681063733903, 'validation_fraction': 0.3522700355940548}]
function_evaluation time 0.367448 value -0.909890 suggestion {'alpha': 1.6141399233527927e-05, 'batch_size': 120, 'beta_1': 0.9118313465899606, 'beta_2': 0.9991836180301912, 'epsilon': 3.806099135654774e-09, 'hidden_layer_sizes': 187, 'learning_rate_init': 0.0026192605764600997, 'tol': 0.00014664681063733903, 'validation_fraction': 0.3522700355940548}
observation time 0.000001, current best -0.909890 at iter 0
suggestion time taken 0.004127 iter 1 next_points [{'alpha': 3.456175024987329, 'batch_size': 234, 'beta_1': 0.9284102824648688, 'beta_2': 0.9999978740005854, 'epsilon': 5.341766328122095e-07, 'hidden_layer_sizes': 122, 'learning_rate_init': 7.202176918252435e-05, 'tol': 0.0003407059454299872, 'validation_fraction': 0.688686577513775}]
/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

function_evaluation time 0.115734 value -0.549451 suggestion {'alpha': 3.456175024987329, 'batch_size': 234, 'beta_1': 0.9284102824648688, 'beta_2': 0.9999978740005854, 'epsilon': 5.341766328122095e-07, 'hidden_layer_sizes': 122, 'learning_rate_init': 7.202176918252435e-05, 'tol': 0.0003407059454299872, 'validation_fraction': 0.688686577513775}
observation time 0.000000, current best -0.909890 at iter 1
suggestion time taken 0.004163 iter 2 next_points [{'alpha': 1.1711282904258613e-05, 'batch_size': 220, 'beta_1': 0.9290075988985539, 'beta_2': 0.9999776123528261, 'epsilon': 5.301874679715236e-09, 'hidden_layer_sizes': 56, 'learning_rate_init': 1.0757962755428833e-05, 'tol': 1.5163785662058178e-05, 'validation_fraction': 0.2320335373310199}]
function_evaluation time 0.122184 value -0.472527 suggestion {'alpha': 1.1711282904258613e-05, 'batch_size': 220, 'beta_1': 0.9290075988985539, 'beta_2': 0.9999776123528261, 'epsilon': 5.301874679715236e-09, 'hidden_layer_sizes': 56, 'learning_rate_init': 1.0757962755428833e-05, 'tol': 1.5163785662058178e-05, 'validation_fraction': 0.2320335373310199}
observation time 0.000000, current best -0.909890 at iter 2
suggestion time taken 0.004144 iter 3 next_points [{'alpha': 0.0005482998732387303, 'batch_size': 105, 'beta_1': 0.7791306977400863, 'beta_2': 0.9991847269710951, 'epsilon': 3.995587402010989e-09, 'hidden_layer_sizes': 170, 'learning_rate_init': 0.04294038876777303, 'tol': 0.0022468358438946907, 'validation_fraction': 0.8151476383193699}]
/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

function_evaluation time 0.243529 value -0.841758 suggestion {'alpha': 0.0005482998732387303, 'batch_size': 105, 'beta_1': 0.7791306977400863, 'beta_2': 0.9991847269710951, 'epsilon': 3.995587402010989e-09, 'hidden_layer_sizes': 170, 'learning_rate_init': 0.04294038876777303, 'tol': 0.0022468358438946907, 'validation_fraction': 0.8151476383193699}
observation time 0.000001, current best -0.909890 at iter 3
suggestion time taken 0.004148 iter 4 next_points [{'alpha': 0.0003999011303355112, 'batch_size': 101, 'beta_1': 0.9003392530338242, 'beta_2': 0.9998599568057492, 'epsilon': 1.7233410558500418e-07, 'hidden_layer_sizes': 91, 'learning_rate_init': 0.06867687684479742, 'tol': 0.01799855706135038, 'validation_fraction': 0.1683926250121331}]
function_evaluation time 0.226529 value -0.837363 suggestion {'alpha': 0.0003999011303355112, 'batch_size': 101, 'beta_1': 0.9003392530338242, 'beta_2': 0.9998599568057492, 'epsilon': 1.7233410558500418e-07, 'hidden_layer_sizes': 91, 'learning_rate_init': 0.06867687684479742, 'tol': 0.01799855706135038, 'validation_fraction': 0.1683926250121331}
observation time 0.000000, current best -0.909890 at iter 4
suggestion time taken 0.004119 iter 5 next_points [{'alpha': 1.570314919008635e-05, 'batch_size': 150, 'beta_1': 0.8923629067505848, 'beta_2': 0.9999801089179778, 'epsilon': 9.700425736005237e-09, 'hidden_layer_sizes': 123, 'learning_rate_init': 0.015951656280693845, 'tol': 0.014092059911605557, 'validation_fraction': 0.6152020277761472}]
function_evaluation time 0.198623 value -0.901099 suggestion {'alpha': 1.570314919008635e-05, 'batch_size': 150, 'beta_1': 0.8923629067505848, 'beta_2': 0.9999801089179778, 'epsilon': 9.700425736005237e-09, 'hidden_layer_sizes': 123, 'learning_rate_init': 0.015951656280693845, 'tol': 0.014092059911605557, 'validation_fraction': 0.6152020277761472}
observation time 0.000003, current best -0.909890 at iter 5
suggestion time taken 0.004251 iter 6 next_points [{'alpha': 9.335778209137999e-05, 'batch_size': 23, 'beta_1': 0.9738131129723653, 'beta_2': 0.9999937621894072, 'epsilon': 5.663955312169408e-08, 'hidden_layer_sizes': 185, 'learning_rate_init': 2.772671693794105e-05, 'tol': 0.006116936957088273, 'validation_fraction': 0.11120668267314571}]
function_evaluation time 0.669137 value -0.641758 suggestion {'alpha': 9.335778209137999e-05, 'batch_size': 23, 'beta_1': 0.9738131129723653, 'beta_2': 0.9999937621894072, 'epsilon': 5.663955312169408e-08, 'hidden_layer_sizes': 185, 'learning_rate_init': 2.772671693794105e-05, 'tol': 0.006116936957088273, 'validation_fraction': 0.11120668267314571}
observation time 0.000000, current best -0.909890 at iter 6
suggestion time taken 0.004123 iter 7 next_points [{'alpha': 0.2549077376857045, 'batch_size': 64, 'beta_1': 0.6712737497490571, 'beta_2': 0.9999722227069283, 'epsilon': 1.069092211495336e-08, 'hidden_layer_sizes': 66, 'learning_rate_init': 7.414466529424179e-05, 'tol': 0.02456239275598923, 'validation_fraction': 0.13128644672912773}]
function_evaluation time 0.158121 value -0.523077 suggestion {'alpha': 0.2549077376857045, 'batch_size': 64, 'beta_1': 0.6712737497490571, 'beta_2': 0.9999722227069283, 'epsilon': 1.069092211495336e-08, 'hidden_layer_sizes': 66, 'learning_rate_init': 7.414466529424179e-05, 'tol': 0.02456239275598923, 'validation_fraction': 0.13128644672912773}
observation time 0.000000, current best -0.909890 at iter 7
suggestion time taken 0.003342 iter 8 next_points [{'alpha': 0.0010473019435819156, 'batch_size': 134, 'beta_1': 0.9382792693343266, 'beta_2': 0.9999979504663039, 'epsilon': 4.599944170994296e-07, 'hidden_layer_sizes': 64, 'learning_rate_init': 0.062444536326757547, 'tol': 2.2652891824990514e-05, 'validation_fraction': 0.4907429953686779}]
function_evaluation time 0.173736 value -0.797802 suggestion {'alpha': 0.0010473019435819156, 'batch_size': 134, 'beta_1': 0.9382792693343266, 'beta_2': 0.9999979504663039, 'epsilon': 4.599944170994296e-07, 'hidden_layer_sizes': 64, 'learning_rate_init': 0.062444536326757547, 'tol': 2.2652891824990514e-05, 'validation_fraction': 0.4907429953686779}
observation time 0.000000, current best -0.909890 at iter 8
suggestion time taken 0.004153 iter 9 next_points [{'alpha': 0.00037918518510319073, 'batch_size': 168, 'beta_1': 0.9876940038492037, 'beta_2': 0.998891606453431, 'epsilon': 4.3717000936660726e-08, 'hidden_layer_sizes': 138, 'learning_rate_init': 3.040829731397877e-05, 'tol': 0.003323599970692614, 'validation_fraction': 0.4233577040160778}]
function_evaluation time 0.164594 value -0.402198 suggestion {'alpha': 0.00037918518510319073, 'batch_size': 168, 'beta_1': 0.9876940038492037, 'beta_2': 0.998891606453431, 'epsilon': 4.3717000936660726e-08, 'hidden_layer_sizes': 138, 'learning_rate_init': 3.040829731397877e-05, 'tol': 0.003323599970692614, 'validation_fraction': 0.4233577040160778}
observation time 0.000000, current best -0.909890 at iter 9
suggestion time taken 0.004153 iter 10 next_points [{'alpha': 0.00048305642359759373, 'batch_size': 205, 'beta_1': 0.7859509963105576, 'beta_2': 0.9999283629293974, 'epsilon': 2.2690319242986077e-09, 'hidden_layer_sizes': 68, 'learning_rate_init': 4.164226141135345e-05, 'tol': 1.1441455146998911e-05, 'validation_fraction': 0.3942970248075996}]
function_evaluation time 0.125133 value -0.582418 suggestion {'alpha': 0.00048305642359759373, 'batch_size': 205, 'beta_1': 0.7859509963105576, 'beta_2': 0.9999283629293974, 'epsilon': 2.2690319242986077e-09, 'hidden_layer_sizes': 68, 'learning_rate_init': 4.164226141135345e-05, 'tol': 1.1441455146998911e-05, 'validation_fraction': 0.3942970248075996}
observation time 0.000000, current best -0.909890 at iter 10
suggestion time taken 0.004148 iter 11 next_points [{'alpha': 0.026264311175506205, 'batch_size': 176, 'beta_1': 0.9885742892316339, 'beta_2': 0.9991666045081696, 'epsilon': 2.168521837880477e-09, 'hidden_layer_sizes': 113, 'learning_rate_init': 0.0007197342038967421, 'tol': 0.09387119326305933, 'validation_fraction': 0.5119267127929812}]
function_evaluation time 0.189217 value -0.696703 suggestion {'alpha': 0.026264311175506205, 'batch_size': 176, 'beta_1': 0.9885742892316339, 'beta_2': 0.9991666045081696, 'epsilon': 2.168521837880477e-09, 'hidden_layer_sizes': 113, 'learning_rate_init': 0.0007197342038967421, 'tol': 0.09387119326305933, 'validation_fraction': 0.5119267127929812}
observation time 0.000000, current best -0.909890 at iter 11
suggestion time taken 0.004132 iter 12 next_points [{'alpha': 0.24320751637981075, 'batch_size': 186, 'beta_1': 0.9554749321961692, 'beta_2': 0.9037428469810013, 'epsilon': 1.4864384391124652e-09, 'hidden_layer_sizes': 107, 'learning_rate_init': 0.00016331269876233967, 'tol': 0.0002822175522884839, 'validation_fraction': 0.7830465355428741}]
/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

function_evaluation time 0.108224 value -0.582418 suggestion {'alpha': 0.24320751637981075, 'batch_size': 186, 'beta_1': 0.9554749321961692, 'beta_2': 0.9037428469810013, 'epsilon': 1.4864384391124652e-09, 'hidden_layer_sizes': 107, 'learning_rate_init': 0.00016331269876233967, 'tol': 0.0002822175522884839, 'validation_fraction': 0.7830465355428741}
observation time 0.000001, current best -0.909890 at iter 12
suggestion time taken 0.004117 iter 13 next_points [{'alpha': 0.009890409078730449, 'batch_size': 211, 'beta_1': 0.9545018445494939, 'beta_2': 0.9504508889321853, 'epsilon': 1.6341686026389625e-09, 'hidden_layer_sizes': 117, 'learning_rate_init': 0.00027293754442155597, 'tol': 0.004162174093521161, 'validation_fraction': 0.6477925057483248}]
/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

function_evaluation time 0.176292 value -0.571429 suggestion {'alpha': 0.009890409078730449, 'batch_size': 211, 'beta_1': 0.9545018445494939, 'beta_2': 0.9504508889321853, 'epsilon': 1.6341686026389625e-09, 'hidden_layer_sizes': 117, 'learning_rate_init': 0.00027293754442155597, 'tol': 0.004162174093521161, 'validation_fraction': 0.6477925057483248}
observation time 0.000000, current best -0.909890 at iter 13
suggestion time taken 0.004107 iter 14 next_points [{'alpha': 3.1075141917533e-05, 'batch_size': 62, 'beta_1': 0.9426455287997687, 'beta_2': 0.9994995208928326, 'epsilon': 2.316393235514483e-08, 'hidden_layer_sizes': 51, 'learning_rate_init': 6.923542652737711e-05, 'tol': 0.05572443288456243, 'validation_fraction': 0.6195154585695909}]
function_evaluation time 0.129719 value -0.520879 suggestion {'alpha': 3.1075141917533e-05, 'batch_size': 62, 'beta_1': 0.9426455287997687, 'beta_2': 0.9994995208928326, 'epsilon': 2.316393235514483e-08, 'hidden_layer_sizes': 51, 'learning_rate_init': 6.923542652737711e-05, 'tol': 0.05572443288456243, 'validation_fraction': 0.6195154585695909}
observation time 0.000001, current best -0.909890 at iter 14
saving meta data: {'args': {'--uuid': 'f83e600d2a0051d2a908334f1b0f09a7', '-db-root': '/home/ryedida/smoothness-hpo/bbo_challenge_starter_kit/output', '--opt-root': '/home/ryedida/smoothness-hpo/bbo_challenge_starter_kit/example_submissions', '--data-root': None, '--db': 'run_20240113_043333', '--opt': 'random', '--data': 'breast', '--classifier': 'MLP-adam', '--metric': 'acc', '--calls': 15, '--suggestions': 1, '--jobs-file': None, '--verbose': False, 'dry_run': False, 'rev': 'a376313', 'opt_rev': '0.0.8'}, 'signature': ([{'alpha': 0.019628224813442792, 'batch_size': 182, 'beta_1': 0.9410202200271762, 'beta_2': 0.9998021557676793, 'epsilon': 1.8662266976518e-08, 'hidden_layer_sizes': 147, 'learning_rate_init': 0.0005627932047415167, 'tol': 0.03690557729213761, 'validation_fraction': 0.8846827852548593}, {'alpha': 0.001998246739232944, 'batch_size': 200, 'beta_1': 0.919111482530466, 'beta_2': 0.9998488260436156, 'epsilon': 5.981221901152555e-07, 'hidden_layer_sizes': 61, 'learning_rate_init': 2.2310905607443014e-05, 'tol': 1.2046852412030316e-05, 'validation_fraction': 0.8117896445826539}, {'alpha': 0.46659545670218433, 'batch_size': 219, 'beta_1': 0.9889789783750891, 'beta_2': 0.9999896868093284, 'epsilon': 2.4234724484675948e-08, 'hidden_layer_sizes': 167, 'learning_rate_init': 2.972334644335654e-05, 'tol': 0.0036281404040243792, 'validation_fraction': 0.17260651658522078}, {'alpha': 4.656005689076002, 'batch_size': 135, 'beta_1': 0.870503881627747, 'beta_2': 0.9948873266941017, 'epsilon': 2.1023308743480125e-07, 'hidden_layer_sizes': 118, 'learning_rate_init': 0.001878173875716191, 'tol': 1.1889379831773006e-05, 'validation_fraction': 0.6264327093752792}, {'alpha': 0.04705159350400542, 'batch_size': 158, 'beta_1': 0.9870884262730957, 'beta_2': 0.9999596874382349, 'epsilon': 1.1981845126013875e-08, 'hidden_layer_sizes': 116, 'learning_rate_init': 0.006173405204074311, 'tol': 1.7414134181586194e-05, 'validation_fraction': 0.6754299026638921}], [-0.676923076923077, -0.567032967032967, -0.545054945054945, -0.7824175824175824, -0.843956043956044])}
saving results
saving timing
saving suggest log
done
