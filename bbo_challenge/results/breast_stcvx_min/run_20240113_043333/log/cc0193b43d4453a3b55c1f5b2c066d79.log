running: {'--uuid': 'cc0193b43d4453a3b55c1f5b2c066d79', '-db-root': '/home/ryedida/smoothness-hpo/bbo_challenge_starter_kit/output', '--opt-root': '/home/ryedida/smoothness-hpo/bbo_challenge_starter_kit/example_submissions', '--data-root': None, '--db': 'run_20240113_043333', '--opt': 'random', '--data': 'breast', '--classifier': 'MLP-adam', '--metric': 'nll', '--calls': 15, '--suggestions': 1, '--jobs-file': None, '--verbose': False, 'dry_run': False, 'rev': 'a376313', 'opt_rev': '0.0.8'}
cmd: python /home/ryedida/.local/bin/bayesmark-exp -c MLP-adam -d breast -o random -u cc0193b43d4453a3b55c1f5b2c066d79 -m nll -n 15 -p 1 -dir /home/ryedida/smoothness-hpo/bbo_challenge_starter_kit/output -b run_20240113_043333
/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

computed signature: ([{'alpha': 0.019628224813442792, 'batch_size': 182, 'beta_1': 0.9410202200271762, 'beta_2': 0.9998021557676793, 'epsilon': 1.8662266976518e-08, 'hidden_layer_sizes': 147, 'learning_rate_init': 0.0005627932047415167, 'tol': 0.03690557729213761, 'validation_fraction': 0.8846827852548593}, {'alpha': 0.001998246739232944, 'batch_size': 200, 'beta_1': 0.919111482530466, 'beta_2': 0.9998488260436156, 'epsilon': 5.981221901152555e-07, 'hidden_layer_sizes': 61, 'learning_rate_init': 2.2310905607443014e-05, 'tol': 1.2046852412030316e-05, 'validation_fraction': 0.8117896445826539}, {'alpha': 0.46659545670218433, 'batch_size': 219, 'beta_1': 0.9889789783750891, 'beta_2': 0.9999896868093284, 'epsilon': 2.4234724484675948e-08, 'hidden_layer_sizes': 167, 'learning_rate_init': 2.972334644335654e-05, 'tol': 0.0036281404040243792, 'validation_fraction': 0.17260651658522078}, {'alpha': 4.656005689076002, 'batch_size': 135, 'beta_1': 0.870503881627747, 'beta_2': 0.9948873266941017, 'epsilon': 2.1023308743480125e-07, 'hidden_layer_sizes': 118, 'learning_rate_init': 0.001878173875716191, 'tol': 1.1889379831773006e-05, 'validation_fraction': 0.6264327093752792}, {'alpha': 0.04705159350400542, 'batch_size': 158, 'beta_1': 0.9870884262730957, 'beta_2': 0.9999596874382349, 'epsilon': 1.1981845126013875e-08, 'hidden_layer_sizes': 116, 'learning_rate_init': 0.006173405204074311, 'tol': 1.7414134181586194e-05, 'validation_fraction': 0.6754299026638921}], [8.104566516477803, 14.643876498263253, 12.332285721418042, 5.269783589486869, 3.4904310816393633])
/home/ryedida/.local/lib/python3.8/site-packages/bayesmark/experiment.py:469: UserWarning: Baselines not found. Will not log intermediate scores.
  warnings.warn("Baselines not found. Will not log intermediate scores.")

starting sklearn study random MLP-adam breast nll 15 1
with data root: None
suggestion time taken 0.004227 iter 0 next_points [{'alpha': 0.009195255573731565, 'batch_size': 199, 'beta_1': 0.9551668546776722, 'beta_2': 0.999561442542719, 'epsilon': 9.28195846238248e-09, 'hidden_layer_sizes': 89, 'learning_rate_init': 0.06082660848035941, 'tol': 0.024700688538956697, 'validation_fraction': 0.3266438338203327}]
function_evaluation time 0.210249 value 0.686288 suggestion {'alpha': 0.009195255573731565, 'batch_size': 199, 'beta_1': 0.9551668546776722, 'beta_2': 0.999561442542719, 'epsilon': 9.28195846238248e-09, 'hidden_layer_sizes': 89, 'learning_rate_init': 0.06082660848035941, 'tol': 0.024700688538956697, 'validation_fraction': 0.3266438338203327}
observation time 0.000001, current best 0.686288 at iter 0
suggestion time taken 0.004103 iter 1 next_points [{'alpha': 0.210014528141877, 'batch_size': 104, 'beta_1': 0.9359925955814313, 'beta_2': 0.999907746332291, 'epsilon': 2.6236998538167978e-09, 'hidden_layer_sizes': 151, 'learning_rate_init': 0.09660991669390662, 'tol': 0.0008991998199931455, 'validation_fraction': 0.4300231897316911}]
function_evaluation time 0.272283 value 0.813396 suggestion {'alpha': 0.210014528141877, 'batch_size': 104, 'beta_1': 0.9359925955814313, 'beta_2': 0.999907746332291, 'epsilon': 2.6236998538167978e-09, 'hidden_layer_sizes': 151, 'learning_rate_init': 0.09660991669390662, 'tol': 0.0008991998199931455, 'validation_fraction': 0.4300231897316911}
observation time 0.000000, current best 0.686288 at iter 1
suggestion time taken 0.004110 iter 2 next_points [{'alpha': 1.4176572909531728e-05, 'batch_size': 224, 'beta_1': 0.9106297689755389, 'beta_2': 0.9218559668560274, 'epsilon': 1.6197741420816305e-08, 'hidden_layer_sizes': 167, 'learning_rate_init': 0.0018468805727554175, 'tol': 0.00040913844394511073, 'validation_fraction': 0.6002686029722353}]
/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

function_evaluation time 0.355239 value 0.578774 suggestion {'alpha': 1.4176572909531728e-05, 'batch_size': 224, 'beta_1': 0.9106297689755389, 'beta_2': 0.9218559668560274, 'epsilon': 1.6197741420816305e-08, 'hidden_layer_sizes': 167, 'learning_rate_init': 0.0018468805727554175, 'tol': 0.00040913844394511073, 'validation_fraction': 0.6002686029722353}
observation time 0.000000, current best 0.578774 at iter 2
suggestion time taken 0.004090 iter 3 next_points [{'alpha': 0.01956032812539, 'batch_size': 41, 'beta_1': 0.5182814743500793, 'beta_2': 0.9999419027117373, 'epsilon': 2.9111940744759423e-08, 'hidden_layer_sizes': 186, 'learning_rate_init': 0.037749370608643355, 'tol': 0.0771182018579766, 'validation_fraction': 0.5993024298181077}]
function_evaluation time 0.234068 value 0.495150 suggestion {'alpha': 0.01956032812539, 'batch_size': 41, 'beta_1': 0.5182814743500793, 'beta_2': 0.9999419027117373, 'epsilon': 2.9111940744759423e-08, 'hidden_layer_sizes': 186, 'learning_rate_init': 0.037749370608643355, 'tol': 0.0771182018579766, 'validation_fraction': 0.5993024298181077}
observation time 0.000001, current best 0.495150 at iter 3
suggestion time taken 0.004092 iter 4 next_points [{'alpha': 0.0002685697860252305, 'batch_size': 196, 'beta_1': 0.7524603873829135, 'beta_2': 0.9949930034239666, 'epsilon': 1.0707995261409167e-09, 'hidden_layer_sizes': 181, 'learning_rate_init': 3.187211814877819e-05, 'tol': 0.001488920682926694, 'validation_fraction': 0.6533284952952841}]
/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

function_evaluation time 0.224511 value 20.629823 suggestion {'alpha': 0.0002685697860252305, 'batch_size': 196, 'beta_1': 0.7524603873829135, 'beta_2': 0.9949930034239666, 'epsilon': 1.0707995261409167e-09, 'hidden_layer_sizes': 181, 'learning_rate_init': 3.187211814877819e-05, 'tol': 0.001488920682926694, 'validation_fraction': 0.6533284952952841}
observation time 0.000002, current best 0.495150 at iter 4
suggestion time taken 0.004225 iter 5 next_points [{'alpha': 5.626132709757369e-05, 'batch_size': 202, 'beta_1': 0.544340452623798, 'beta_2': 0.9999439230391792, 'epsilon': 7.315494041120736e-07, 'hidden_layer_sizes': 111, 'learning_rate_init': 0.0015902754469410043, 'tol': 0.0027889628746161146, 'validation_fraction': 0.2064269042280895}]
function_evaluation time 0.303061 value 0.486330 suggestion {'alpha': 5.626132709757369e-05, 'batch_size': 202, 'beta_1': 0.544340452623798, 'beta_2': 0.9999439230391792, 'epsilon': 7.315494041120736e-07, 'hidden_layer_sizes': 111, 'learning_rate_init': 0.0015902754469410043, 'tol': 0.0027889628746161146, 'validation_fraction': 0.2064269042280895}
observation time 0.000000, current best 0.486330 at iter 5
suggestion time taken 0.004075 iter 6 next_points [{'alpha': 0.00014365465540367323, 'batch_size': 117, 'beta_1': 0.8839476233608335, 'beta_2': 0.999196812956843, 'epsilon': 1.460098319401911e-07, 'hidden_layer_sizes': 73, 'learning_rate_init': 0.004798651706343522, 'tol': 0.001876935272365616, 'validation_fraction': 0.29129815465944026}]
function_evaluation time 0.237100 value 0.668280 suggestion {'alpha': 0.00014365465540367323, 'batch_size': 117, 'beta_1': 0.8839476233608335, 'beta_2': 0.999196812956843, 'epsilon': 1.460098319401911e-07, 'hidden_layer_sizes': 73, 'learning_rate_init': 0.004798651706343522, 'tol': 0.001876935272365616, 'validation_fraction': 0.29129815465944026}
observation time 0.000000, current best 0.486330 at iter 6
suggestion time taken 0.004092 iter 7 next_points [{'alpha': 0.02100222021444076, 'batch_size': 161, 'beta_1': 0.5732659899459615, 'beta_2': 0.9998223579673968, 'epsilon': 8.259089191167135e-07, 'hidden_layer_sizes': 68, 'learning_rate_init': 0.00020401266856447242, 'tol': 3.481668626531417e-05, 'validation_fraction': 0.8353651144136202}]
/home/ryedida/.local/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:603: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped
  warnings.warn(

function_evaluation time 0.103860 value 14.178301 suggestion {'alpha': 0.02100222021444076, 'batch_size': 161, 'beta_1': 0.5732659899459615, 'beta_2': 0.9998223579673968, 'epsilon': 8.259089191167135e-07, 'hidden_layer_sizes': 68, 'learning_rate_init': 0.00020401266856447242, 'tol': 3.481668626531417e-05, 'validation_fraction': 0.8353651144136202}
observation time 0.000000, current best 0.486330 at iter 7
suggestion time taken 0.004089 iter 8 next_points [{'alpha': 0.0003130223024491681, 'batch_size': 88, 'beta_1': 0.8968836335793389, 'beta_2': 0.9999970350072422, 'epsilon': 2.9244058161924836e-08, 'hidden_layer_sizes': 65, 'learning_rate_init': 0.00010039689300605771, 'tol': 0.00029744932818506135, 'validation_fraction': 0.10549055656272699}]
function_evaluation time 0.141298 value 14.976290 suggestion {'alpha': 0.0003130223024491681, 'batch_size': 88, 'beta_1': 0.8968836335793389, 'beta_2': 0.9999970350072422, 'epsilon': 2.9244058161924836e-08, 'hidden_layer_sizes': 65, 'learning_rate_init': 0.00010039689300605771, 'tol': 0.00029744932818506135, 'validation_fraction': 0.10549055656272699}
observation time 0.000000, current best 0.486330 at iter 8
suggestion time taken 0.003178 iter 9 next_points [{'alpha': 0.008167673866707569, 'batch_size': 107, 'beta_1': 0.7487527373108811, 'beta_2': 0.9563289534451098, 'epsilon': 4.631854484797827e-09, 'hidden_layer_sizes': 134, 'learning_rate_init': 0.0005766652483399562, 'tol': 2.4866728619392138e-05, 'validation_fraction': 0.2150592888851714}]
function_evaluation time 0.385111 value 0.446938 suggestion {'alpha': 0.008167673866707569, 'batch_size': 107, 'beta_1': 0.7487527373108811, 'beta_2': 0.9563289534451098, 'epsilon': 4.631854484797827e-09, 'hidden_layer_sizes': 134, 'learning_rate_init': 0.0005766652483399562, 'tol': 2.4866728619392138e-05, 'validation_fraction': 0.2150592888851714}
observation time 0.000000, current best 0.446938 at iter 9
suggestion time taken 0.004068 iter 10 next_points [{'alpha': 0.07022853980193368, 'batch_size': 13, 'beta_1': 0.9863093181451436, 'beta_2': 0.9936974068569432, 'epsilon': 2.993988652137451e-07, 'hidden_layer_sizes': 128, 'learning_rate_init': 0.0014908453620548485, 'tol': 0.0009545679816477786, 'validation_fraction': 0.13939807491882467}]
function_evaluation time 0.945552 value 0.637598 suggestion {'alpha': 0.07022853980193368, 'batch_size': 13, 'beta_1': 0.9863093181451436, 'beta_2': 0.9936974068569432, 'epsilon': 2.993988652137451e-07, 'hidden_layer_sizes': 128, 'learning_rate_init': 0.0014908453620548485, 'tol': 0.0009545679816477786, 'validation_fraction': 0.13939807491882467}
observation time 0.000001, current best 0.446938 at iter 10
suggestion time taken 0.003875 iter 11 next_points [{'alpha': 0.03523156545760386, 'batch_size': 111, 'beta_1': 0.5611733743271917, 'beta_2': 0.9977886040555043, 'epsilon': 2.57550665713041e-07, 'hidden_layer_sizes': 192, 'learning_rate_init': 0.0012039839399162922, 'tol': 1.9469904937780234e-05, 'validation_fraction': 0.6701642715375471}]
function_evaluation time 0.319283 value 0.375732 suggestion {'alpha': 0.03523156545760386, 'batch_size': 111, 'beta_1': 0.5611733743271917, 'beta_2': 0.9977886040555043, 'epsilon': 2.57550665713041e-07, 'hidden_layer_sizes': 192, 'learning_rate_init': 0.0012039839399162922, 'tol': 1.9469904937780234e-05, 'validation_fraction': 0.6701642715375471}
observation time 0.000001, current best 0.375732 at iter 11
suggestion time taken 0.004258 iter 12 next_points [{'alpha': 3.5530597135103203, 'batch_size': 33, 'beta_1': 0.9205593597400701, 'beta_2': 0.9979450903328343, 'epsilon': 8.429070262263663e-09, 'hidden_layer_sizes': 106, 'learning_rate_init': 0.00015821949887295838, 'tol': 1.8160190697051007e-05, 'validation_fraction': 0.5539424160673195}]
function_evaluation time 0.322446 value 5.258227 suggestion {'alpha': 3.5530597135103203, 'batch_size': 33, 'beta_1': 0.9205593597400701, 'beta_2': 0.9979450903328343, 'epsilon': 8.429070262263663e-09, 'hidden_layer_sizes': 106, 'learning_rate_init': 0.00015821949887295838, 'tol': 1.8160190697051007e-05, 'validation_fraction': 0.5539424160673195}
observation time 0.000001, current best 0.375732 at iter 12
suggestion time taken 0.004319 iter 13 next_points [{'alpha': 0.6843789373524857, 'batch_size': 189, 'beta_1': 0.882739001300679, 'beta_2': 0.9962727407592706, 'epsilon': 1.3211704340173212e-07, 'hidden_layer_sizes': 151, 'learning_rate_init': 0.009986378396500404, 'tol': 0.022038817415383948, 'validation_fraction': 0.5536205339726613}]
function_evaluation time 0.205151 value 0.971099 suggestion {'alpha': 0.6843789373524857, 'batch_size': 189, 'beta_1': 0.882739001300679, 'beta_2': 0.9962727407592706, 'epsilon': 1.3211704340173212e-07, 'hidden_layer_sizes': 151, 'learning_rate_init': 0.009986378396500404, 'tol': 0.022038817415383948, 'validation_fraction': 0.5536205339726613}
observation time 0.000001, current best 0.375732 at iter 13
suggestion time taken 0.004276 iter 14 next_points [{'alpha': 0.8680721522733206, 'batch_size': 129, 'beta_1': 0.7639692913070177, 'beta_2': 0.9971617325006571, 'epsilon': 7.817287295363994e-09, 'hidden_layer_sizes': 186, 'learning_rate_init': 0.0001088500437471975, 'tol': 2.955042559109512e-05, 'validation_fraction': 0.2584372045817135}]
function_evaluation time 0.454976 value 9.625168 suggestion {'alpha': 0.8680721522733206, 'batch_size': 129, 'beta_1': 0.7639692913070177, 'beta_2': 0.9971617325006571, 'epsilon': 7.817287295363994e-09, 'hidden_layer_sizes': 186, 'learning_rate_init': 0.0001088500437471975, 'tol': 2.955042559109512e-05, 'validation_fraction': 0.2584372045817135}
observation time 0.000001, current best 0.375732 at iter 14
saving meta data: {'args': {'--uuid': 'cc0193b43d4453a3b55c1f5b2c066d79', '-db-root': '/home/ryedida/smoothness-hpo/bbo_challenge_starter_kit/output', '--opt-root': '/home/ryedida/smoothness-hpo/bbo_challenge_starter_kit/example_submissions', '--data-root': None, '--db': 'run_20240113_043333', '--opt': 'random', '--data': 'breast', '--classifier': 'MLP-adam', '--metric': 'nll', '--calls': 15, '--suggestions': 1, '--jobs-file': None, '--verbose': False, 'dry_run': False, 'rev': 'a376313', 'opt_rev': '0.0.8'}, 'signature': ([{'alpha': 0.019628224813442792, 'batch_size': 182, 'beta_1': 0.9410202200271762, 'beta_2': 0.9998021557676793, 'epsilon': 1.8662266976518e-08, 'hidden_layer_sizes': 147, 'learning_rate_init': 0.0005627932047415167, 'tol': 0.03690557729213761, 'validation_fraction': 0.8846827852548593}, {'alpha': 0.001998246739232944, 'batch_size': 200, 'beta_1': 0.919111482530466, 'beta_2': 0.9998488260436156, 'epsilon': 5.981221901152555e-07, 'hidden_layer_sizes': 61, 'learning_rate_init': 2.2310905607443014e-05, 'tol': 1.2046852412030316e-05, 'validation_fraction': 0.8117896445826539}, {'alpha': 0.46659545670218433, 'batch_size': 219, 'beta_1': 0.9889789783750891, 'beta_2': 0.9999896868093284, 'epsilon': 2.4234724484675948e-08, 'hidden_layer_sizes': 167, 'learning_rate_init': 2.972334644335654e-05, 'tol': 0.0036281404040243792, 'validation_fraction': 0.17260651658522078}, {'alpha': 4.656005689076002, 'batch_size': 135, 'beta_1': 0.870503881627747, 'beta_2': 0.9948873266941017, 'epsilon': 2.1023308743480125e-07, 'hidden_layer_sizes': 118, 'learning_rate_init': 0.001878173875716191, 'tol': 1.1889379831773006e-05, 'validation_fraction': 0.6264327093752792}, {'alpha': 0.04705159350400542, 'batch_size': 158, 'beta_1': 0.9870884262730957, 'beta_2': 0.9999596874382349, 'epsilon': 1.1981845126013875e-08, 'hidden_layer_sizes': 116, 'learning_rate_init': 0.006173405204074311, 'tol': 1.7414134181586194e-05, 'validation_fraction': 0.6754299026638921}], [8.104566516477803, 14.643876498263253, 12.332285721418042, 5.269783589486869, 3.4904310816393633])}
saving results
saving timing
saving suggest log
done
